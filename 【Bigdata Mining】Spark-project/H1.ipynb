{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects of using Spark\n",
    "#### 16300200020 Yanjian Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the linux command to delete the first 76 lines and redirect to the data_new.txt file.Then delete the last 49 lines and redirect to the data_new.txt. We can see that the total numbers of lines are 49611709"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tail -n +76 data_dump.sql > data_tmp.txt\n",
    "wc -l data_tmp.txt    result : 49611758\n",
    "head -49611709 data_tmp.txt > data_new.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce the spark package and configure the spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bigdatalab06'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from itertools import islice\n",
    "import re\n",
    "conf = SparkConf().setAppName(\"turkey\").setMaster(\"local\").set(\"spark.executor.memory\",\"12g\")\n",
    "conf.set(\"spark.executor.cores\",\"4\").set(\"spark.memory.offHeap.enabled\",\"true\")\n",
    "conf.set(\"spark.executor.memoryOverhead\",\"4096\").set(\"spark.memory.offHeap.size\",\"4096\")\n",
    "sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector,Vectors\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer,VectorIndexer\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql as sql\n",
    "\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_4000w.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "# classOfFirstName = data.map(lambda x: x[2]).distinct().persist()\n",
    "# print(classOfFirstName.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(classOfFirstName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classOfLastName = data.map(lambda x: x[3]).distinct().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "spark = SparkSession(sc)\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['K', 'NESLIHAN'], ['K', 'SADET'], ['K', 'GONUL'], ['E', 'MURAT'], ['K', 'SENEM'], ['K', 'ZOHRE'], ['K', 'ARIFE'], ['K', 'AYSE'], ['K', 'MELEK'], ['K', 'YILDIZ']]\n",
      "+------------+----------------+\n",
      "|gender_index|first_name_index|\n",
      "+------------+----------------+\n",
      "|         0.0|           170.0|\n",
      "|         0.0|           748.0|\n",
      "|         0.0|            97.0|\n",
      "|         1.0|            11.0|\n",
      "|         0.0|           397.0|\n",
      "|         0.0|           510.0|\n",
      "|         0.0|           190.0|\n",
      "|         0.0|             2.0|\n",
      "|         0.0|            55.0|\n",
      "|         0.0|           112.0|\n",
      "|         1.0|           375.0|\n",
      "|         1.0|            39.0|\n",
      "|         1.0|            53.0|\n",
      "|         1.0|           107.0|\n",
      "|         1.0|             0.0|\n",
      "|         1.0|            80.0|\n",
      "|         0.0|            40.0|\n",
      "|         1.0|           498.0|\n",
      "|         0.0|          2851.0|\n",
      "|         1.0|           323.0|\n",
      "+------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def label2onehot(label,len_):\n",
    "    onehot = [0] * len_\n",
    "    onehot[int(label)] = 1\n",
    "    return onehot\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "currentdata = data.map(lambda x: [x[6], x[2]])\n",
    "print(currentdata.take(10))\n",
    "# highFre = data.map(lambda x: (x[2], 1)).reduceByKey(lambda x, y: x+y).top(200000, key = lambda x: x[1])\n",
    "# print(data.map(lambda x: x[14]).distinct().count()) # 207482\n",
    "# print(data.map(lambda x: x[14].split()[0]).distinct().count()) # 119346\n",
    "schema = ['gender','first_name']\n",
    "# forfit = spark.createDataFrame(currentdata.filter(lambda x: x[1] in highFre), schema)\n",
    "df = spark.createDataFrame(currentdata, schema)\n",
    "\n",
    "indexer = [StringIndexer(inputCol = column, outputCol = column + \"_index\") for column in schema]\n",
    "# onehotindexer = OneHotEncoderEstimator(inputCols = [column + \"_index\" for column in schema], outputCols = [column + \"_onehot\" for column in schema])\n",
    "pipeline = Pipeline(stages = indexer)\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.drop(*schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_n(l_, n):\n",
    "    count = 0\n",
    "    record = {}\n",
    "    for item in l_:\n",
    "        record[count] = item\n",
    "        count += 1\n",
    "    record  = sorted(record.items() , key = lambda x:x[1], reverse = True)\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append(record[i][0])\n",
    "    return result\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "vectorForm = df.rdd.map(lambda row: (row[0], Vectors.dense(row[1:2])))\n",
    "Vectordf = spark.createDataFrame(vectorForm, ['label', 'features'])\n",
    "trainData, validData, testData = Vectordf.rdd.randomSplit([0.7,0.1,0.2])\n",
    "# linear regression\n",
    "# lr = LogisticRegression(regParam=0.1, elasticNetParam=1.0, family=\"multinomial\")\n",
    "# lrmodel = lr.fit(trainData.toDF())\n",
    "# randomforest\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=2)\n",
    "rfmodel = rf.fit(trainData.toDF())\n",
    "predictions = rfmodel.transform(testData.toDF())\n",
    "# bayes\n",
    "# nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n",
    "# nbmodel = nb.fit(trainData.toDF())\n",
    "# predictions = nbmodel.transform(testData.toDF())\n",
    "p1 = predictions.select('probability', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         probability|label|\n",
      "+--------------------+-----+\n",
      "|[0.53775505166535...|  0.0|\n",
      "|[0.66461526699637...|  0.0|\n",
      "|[0.66461526699637...|  1.0|\n",
      "|[0.53775505166535...|  1.0|\n",
      "|[0.53775505166535...|  1.0|\n",
      "|[0.53775505166535...|  1.0|\n",
      "|[0.53775505166535...|  0.0|\n",
      "|[0.65933147469178...|  0.0|\n",
      "|[0.53775505166535...|  1.0|\n",
      "|[0.53775505166535...|  1.0|\n",
      "|[0.64171891715466...|  0.0|\n",
      "|[0.52413583516642...|  1.0|\n",
      "|[0.53775505166535...|  1.0|\n",
      "|[0.66461526699637...|  1.0|\n",
      "|[0.14706768174629...|  1.0|\n",
      "|[0.52413583516642...|  1.0|\n",
      "|[0.14706768174629...|  0.0|\n",
      "|[0.14706768174629...|  1.0|\n",
      "|[0.39298099239038...|  0.0|\n",
      "|[0.52413583516642...|  1.0|\n",
      "|[0.14706768174629...|  1.0|\n",
      "|[0.14706768174629...|  1.0|\n",
      "|[0.14706768174629...|  1.0|\n",
      "|[0.14706768174629...|  0.0|\n",
      "|[0.65933147469178...|  0.0|\n",
      "|[0.64171891715466...|  1.0|\n",
      "|[0.52413583516642...|  1.0|\n",
      "|[0.14706768174629...|  1.0|\n",
      "|[0.14706768174629...|  1.0|\n",
      "|[0.14706768174629...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p1.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6220115951730908, 0.3779884048269092]\n"
     ]
    }
   ],
   "source": [
    "# print(p1.rdd.map(lambda x: (list(enumerate(x[0])), x[1])).take(10))\n",
    "hits = p1.rdd.map(lambda x: (sorted(enumerate(x[0]), key = lambda y: -y[1]), x[1])).map(lambda row: ([x[0] for x in row[0]], row[1])) # 按index 排序\n",
    "hits.take(10)\n",
    "hits = hits.map(lambda x: [ 1 if x[0][i]== x[1] else 0 for i in range(2)])# 计算1-5的hit\n",
    "total = hits.count()\n",
    "hits = [hits.filter(lambda x: x[i] == 1).count()/total for i in range(2)]\n",
    "print(hits)\n",
    "# hits.take(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topn = [p1.rdd.map(lambda x: (hit_n(x[0],1),x[1])).filter(lambda x: int(x[1]) in x[0]).count()/float(top1_pre.count()) for i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(top1_pre.take(10))\n",
    "# print(top1_pre.filter(lambda x: int(x[1]) in x[0]).count()/float(top1_pre.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
