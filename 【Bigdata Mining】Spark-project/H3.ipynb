{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects of using Spark\n",
    "#### 16300200020 Yanjian Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the linux command to delete the first 76 lines and redirect to the data_new.txt file.Then delete the last 49 lines and redirect to the data_new.txt. We can see that the total numbers of lines are 49611709"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tail -n +76 data_dump.sql > data_tmp.txt\n",
    "wc -l data_tmp.txt    result : 49611758\n",
    "head -49611709 data_tmp.txt > data_new.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce the spark package and configure the spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bigdatalab06'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from itertools import islice\n",
    "import re\n",
    "conf = SparkConf().setAppName(\"turkey\").setMaster(\"local\").set(\"spark.executor.memory\",\"12g\")\n",
    "conf.set(\"spark.executor.cores\",\"1\").set(\"spark.memory.offHeap.enabled\",\"true\")\n",
    "conf.set(\"spark.executor.memoryOverhead\",\"4096\").set(\"spark.memory.offHeap.size\",\"4096\")\n",
    "sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector,Vectors\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer,VectorIndexer\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql as sql\n",
    "\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_200w.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "# classOfFirstName = data.map(lambda x: x[2]).distinct().persist()\n",
    "# print(classOfFirstName.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(classOfFirstName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classOfLastName = data.map(lambda x: x[3]).distinct().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "spark = SparkSession(sc)\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ZENGIN', 'ZE', 'OS'], ['YILDIRIM', 'ZO', 'IS'], ['CETIN', 'ES', 'HA'], ['GENC', 'EL', 'IS'], ['YILDIRIM', 'SE', 'MU'], ['YILDIRIM', 'SE', 'MU'], ['GUZEL', 'KA', 'OS'], ['DURMAZ', 'YE', 'SU'], ['ZENGIN', 'MA', 'ME'], ['AKBULUT', 'SA', 'IH']]\n",
      "+---------------+------------------+------------------+\n",
      "|last_name_index|mother_first_index|father_first_index|\n",
      "+---------------+------------------+------------------+\n",
      "|          119.0|               7.0|              13.0|\n",
      "|            8.0|              71.0|               9.0|\n",
      "|           21.0|              27.0|               1.0|\n",
      "|           46.0|              16.0|               9.0|\n",
      "|            8.0|               4.0|               2.0|\n",
      "|            8.0|               4.0|               2.0|\n",
      "|          147.0|              21.0|              13.0|\n",
      "|          207.0|              38.0|              11.0|\n",
      "|          119.0|              23.0|               0.0|\n",
      "|           73.0|               6.0|              52.0|\n",
      "|         4713.0|               0.0|               0.0|\n",
      "|          147.0|              22.0|               4.0|\n",
      "|         4713.0|               0.0|               0.0|\n",
      "|         4713.0|               0.0|               0.0|\n",
      "|          119.0|               8.0|               9.0|\n",
      "|        18248.0|               7.0|               0.0|\n",
      "|          249.0|               8.0|              28.0|\n",
      "|          119.0|              13.0|               4.0|\n",
      "|          119.0|              20.0|               0.0|\n",
      "|          119.0|              15.0|               0.0|\n",
      "+---------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "currentdata = data.map(lambda x: [x[3], x[4][0:2], x[5][0:2]])\n",
    "print(currentdata.take(10))\n",
    "\n",
    "# print(data.map(lambda x: x[14]).distinct().count()) # 207482\n",
    "# print(data.map(lambda x: x[14].split()[0]).distinct().count()) # 119346\n",
    "schema = ['last_name','mother_first','father_first']\n",
    "df = spark.createDataFrame(currentdata, schema)\n",
    "\n",
    "indexer = [StringIndexer(inputCol = column, outputCol = column + \"_index\") for column in schema]\n",
    "# onehotindexer = OneHotEncoderEstimator(inputCols = [column + \"_index\" for column in schema], outputCols = [column + \"_onehot\" for column in schema])\n",
    "pipeline = Pipeline(stages = indexer)\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.drop(*schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "vectorForm = df.rdd.map(lambda row: (row[0], Vectors.dense(row[1:3])))\n",
    "Vectordf = spark.createDataFrame(vectorForm, ['label', 'features'])\n",
    "trainData, validData, testData = Vectordf.rdd.randomSplit([0.7,0.1,0.2])\n",
    "# linear regression\n",
    "# lr = LogisticRegression(regParam=0.1, elasticNetParam=1.0, family=\"multinomial\")\n",
    "# lrmodel = lr.fit(trainData.toDF())\n",
    "# predictions = lrmodel.transform(testData.toDF())\n",
    "# randomforest\n",
    "# rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=2)\n",
    "# rfmodel = rf.fit(trainData.toDF())\n",
    "# predictions = rfmodel.transform(testData.toDF())\n",
    "# bayes\n",
    "nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\")\n",
    "nbmodel = nb.fit(trainData.toDF())\n",
    "predictions = nbmodel.transform(testData.toDF())\n",
    "p1 = predictions.select('probability', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = predictions.select('probability', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|         probability|  label|\n",
      "+--------------------+-------+\n",
      "|[3.32396055889341...|   21.0|\n",
      "|[0.01477152882879...|    8.0|\n",
      "|[0.01477152882879...|    8.0|\n",
      "|[5.63458137170562...|  207.0|\n",
      "|[0.01841569831168...|  119.0|\n",
      "|[0.00454672820864...|  249.0|\n",
      "|[4.31567137653907...|  119.0|\n",
      "|[3.26313555866030...| 1340.0|\n",
      "|[0.01522904020725...|  147.0|\n",
      "|[0.01411537293969...| 1340.0|\n",
      "|[3.06185122435452...|  119.0|\n",
      "|[0.00121803150355...| 1789.0|\n",
      "|[0.02463701494476...|  655.0|\n",
      "|[5.03637807370170...|  282.0|\n",
      "|[0.00809547757635...|  852.0|\n",
      "|[0.00299050192203...|    1.0|\n",
      "|[0.01636529538344...|   41.0|\n",
      "|[3.26313555866030...|  655.0|\n",
      "|[7.40897083751392...|  655.0|\n",
      "|[0.01011708266086...|25695.0|\n",
      "|[0.02078637052082...|   12.0|\n",
      "|[0.00111837008220...|    2.0|\n",
      "|[2.42943512357890...|  852.0|\n",
      "|[0.00793810542262...|   38.0|\n",
      "|[0.01892456334895...|   74.0|\n",
      "|[0.01765684005535...|  162.0|\n",
      "|[0.02017383096703...|  655.0|\n",
      "|[0.01127719815823...|   85.0|\n",
      "|[0.01607711684215...|   85.0|\n",
      "|[0.00952489954363...|  201.0|\n",
      "+--------------------+-------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p1.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(p1.rdd.map(lambda x: (list(enumerate(x[0])), x[1])).take(10))\n",
    "hits = p1.rdd.map(lambda x: (sorted(list(enumerate(x[0]))[0:5], key = lambda y: -y[1]), x[1])).map(lambda row: ([x[0] for x in row[0]], row[1])) # 按index 排序\n",
    "# hits = hits.map(lambda x: [ 1 if x[0][i]== x[1] else 0 for i in range(5)])\n",
    "total = hits.count()\n",
    "hits1 = hits.map(lambda x:  1 if x[0][0]== x[1] else 0).filter(lambda x: x == 1).count()/total\n",
    "print(hits1)\n",
    "hits2 = hits.map(lambda x:  1 if x[0][1]== x[1] else 0).filter(lambda x: x == 1).count()/total\n",
    "print(hits2)\n",
    "hits3 = hits.map(lambda x:  1 if x[0][2]== x[1] else 0).filter(lambda x: x == 1).count()/total\n",
    "print(hits3)\n",
    "hits4 = hits.map(lambda x:  1 if x[0][3]== x[1] else 0).filter(lambda x: x == 1).count()/total\n",
    "print(hits4)\n",
    "hits5 = hits.map(lambda x:  1 if x[0][4]== x[1] else 0).filter(lambda x: x == 1).count()/total\n",
    "print(hits5)\n",
    "# hits.take(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topn = [p1.rdd.map(lambda x: (hit_n(x[0],1),x[1])).filter(lambda x: int(x[1]) in x[0]).count()/float(top1_pre.count()) for i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(top1_pre.take(10))\n",
    "# print(top1_pre.filter(lambda x: int(x[1]) in x[0]).count()/float(top1_pre.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
