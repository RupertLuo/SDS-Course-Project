{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects of using Spark\n",
    "#### 16300200020 Yanjian Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the linux command to delete the first 76 lines and redirect to the data_new.txt file.Then delete the last 49 lines and redirect to the data_new.txt. We can see that the total numbers of lines are 49611709"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tail -n +76 data_dump.sql > data_tmp.txt\n",
    "wc -l data_tmp.txt    result : 49611758\n",
    "head -49611709 data_tmp.txt > data_new.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce the spark package and configure the spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bigdatalab06'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from itertools import islice\n",
    "import re\n",
    "conf = SparkConf().setAppName(\"turkey\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E1. find the eldest\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_new.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "validYearData = data.filter(lambda x : len(x[8].split(\"/\")[-1])==4)\n",
    "male = validYearData.filter(lambda x : x[6] == 'E')\n",
    "year = male.map(lambda x : (\" \".join([x[2], x[3]]),int(x[8][-4:])))\n",
    "name, birthyear = sorted(year.collect(), key = lambda x: x[1])[0]\n",
    "print(\"name\",name)\n",
    "print(\"age\",2019 - int(birthyear))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2. find the max character\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_new.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "name = data.map(lambda x : \"\".join([x[2],x[3]]))\n",
    "count = name.flatMap(lambda x: list(x)).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "letter,freq = sorted(count.collect(), key = lambda x: x[1])[-1]\n",
    "print(\"letter\",letter)\n",
    "print(\"frequency\",freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E3. the distribution of the age\n",
    "def ageLevel(age):\n",
    "    if age <=18:\n",
    "        return '0-18'\n",
    "    if age <= 28:\n",
    "        return '19-28'\n",
    "    if age <= 38:\n",
    "        return '29-38'\n",
    "    if age <= 48:\n",
    "        return '39-48'\n",
    "    if age <= 59:\n",
    "        return '49-59'\n",
    "    return '60+'\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_new.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "validYearData = data.filter(lambda x : len(x[8].split(\"/\")[-1])==4)  # prevent the case like \\95 instead of \\1995\n",
    "year = validYearData.map(lambda x : (x[8][-4:]))\n",
    "age = year.map(lambda x : 2019 - int(x))\n",
    "level = age.map(lambda x: (ageLevel(x),1))\n",
    "levelcount = level.reduceByKey(lambda x, y: x+y)\n",
    "print(levelcount.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E4. birthmonth proportion\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_new.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "month = data.map(lambda x : x[8].split(\"/\")[1])\n",
    "validMonth = month.filter(lambda x: int(x) in range(1,13) if x else False)\n",
    "# print(month.collect()[0])\n",
    "\n",
    "monthCount = validMonth.map(lambda x: (x,1)).reduceByKey(lambda x, y: x+y).persist()\n",
    "totalCount = monthCount.map(lambda x: x[1]).reduce(lambda x,y: x+y)\n",
    "monthPortion = monthCount.mapValues(lambda x: str('%.4f%%' % (x/float(totalCount)*100)))\n",
    "print(monthPortion.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E5. sex proportion\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_new.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "sex = data.map(lambda x : (x[6])).map(lambda x: (x,1))\n",
    "sexCount = sex.reduceByKey(lambda x, y: x+y)\n",
    "totalCount = sexCount.map(lambda x: x[1]).reduce(lambda x,y: x+y)\n",
    "sexPortion = sexCount.mapValues(lambda x: str('%.4f%%' % (x/float(totalCount)*100)))\n",
    "print(\"sex population\",sexCount.collect())\n",
    "print(\"sex proportion\",sexPortion.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N1. top 10 surname for both sex\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_new.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "surnamepair = data.map(lambda x :(x[6],x[3]))\n",
    "male = surnamepair.filter(lambda x : x[0] == \"E\")\n",
    "female = surnamepair.filter(lambda x: x[0] == \"K\")\n",
    "maleCount = male.map(lambda x: (x[1], 1)).reduceByKey(lambda x, y: x + y)\n",
    "femaleCount = female.map(lambda x: (x[1], 1)).reduceByKey(lambda x, y: x + y)\n",
    "print(\"male top 10\",maleCount.top(10, key = lambda x:x[1]))\n",
    "print(\"female top 10\",femaleCount.top(10, key = lambda x:x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N2. cities' average age N3 youngest cities\n",
    "import numpy as np\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_new.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "validYearData = data.filter(lambda x : len(x[8].split(\"/\")[-1])==4)\n",
    "citypair = validYearData.map(lambda x : (x[11], 2019-int(x[8][-4:])))\n",
    "cityAgeMean = citypair.groupByKey().map(lambda x: (x[0], int(np.mean(list(x[1]))))).persist()\n",
    "print(\"cities' average age\",cityAgeMean.collect())\n",
    "print(\"youngest age cities\", sorted(cityAgeMean.collect(),key = lambda x : x[1])[:5])\n",
    "cityAgeMean.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N4. top 3 surname in top 10 cities\n",
    "from heapq import nlargest\n",
    "from operator import add\n",
    "from collections import Counter\n",
    "def counterTupleList(l_:list):\n",
    "    dic = Counter(l_)\n",
    "    keys=dic.keys()\n",
    "    vals=dic.values()\n",
    "    tupleList=[(key,val) for key,val in zip(keys,vals)]\n",
    "    return tupleList\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_new.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "cityNamePair = data.map(lambda x :(x[7], x[3])) \n",
    "top10cityGroup = cityNamePair.groupByKey() \\\n",
    "                            .sortBy(lambda x : len(x[1]),ascending=False) \\\n",
    "                            .zipWithIndex() \\\n",
    "                            .filter(lambda x: x[1] < 10) \\\n",
    "                            .map(lambda x : x[0])\n",
    "top3surname = top10cityGroup.mapValues(lambda x : counterTupleList(x)) \\\n",
    "                            .mapValues(lambda x : nlargest(3, x, key = lambda t : t[1])) \\\n",
    "                            .mapValues(lambda x : [t[0] for t in x])\n",
    "print(\"top 3 surname in top 10 cities\", top3surname.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N5. top 2 month in top 10 cities\n",
    "from heapq import nlargest\n",
    "from collections import Counter\n",
    "def counterTupleList(l_:list):\n",
    "    dic = Counter(l_)\n",
    "    keys=dic.keys()\n",
    "    vals=dic.values()\n",
    "    tupleList=[(key,val) for key,val in zip(keys,vals)]\n",
    "    return tupleList\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_new.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "cityMonthPair = data.map(lambda x :(x[7], x[8].split(\"/\")[1]))\n",
    "top10cityGroup = cityMonthPair.groupByKey() \\\n",
    "                            .sortBy(lambda x : len(x[1]),ascending=False) \\\n",
    "                            .zipWithIndex() \\\n",
    "                            .filter(lambda x: x[1] < 10) \\\n",
    "                            .map(lambda x : x[0])\n",
    "top2month= top10cityGroup.mapValues(lambda x : counterTupleList(x)) \\\n",
    "                            .mapValues(lambda x : nlargest(2, x, key = lambda t : t[1])) \\\n",
    "                            .mapValues(lambda x : [t[0] for t in x])\n",
    "print(\"top 2 month in top 10 cities\", top2month.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 name of male [('MEHMET YILMAZ', 15665), ('MUSTAFA YILMAZ', 13049), ('MEHMET KAYA', 12177), ('MEHMET DEMIR', 11947), ('AHMET YILMAZ', 10137)]\n",
      "top 5 name of female [('FATMA YILMAZ', 17376), ('AYSE YILMAZ', 13475), ('EMINE YILMAZ', 11462), ('FATMA KAYA', 11424), ('FATMA DEMIR', 11207)]\n"
     ]
    }
   ],
   "source": [
    "# N6. top 5 name for male and female\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_new.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "namepair = data.map(lambda x :(x[6],\" \".join([x[2],x[3]])))\n",
    "maleNameCount = namepair.filter(lambda x : x[0] == \"E\").map(lambda x: (x[1],1)).reduceByKey(lambda x, y: x+y)\n",
    "femaleNameCount = namepair.filter(lambda x: x[0] == \"K\").map(lambda x: (x[1],1)).reduceByKey(lambda x, y: x+y)\n",
    "print(\"top 5 name of male\", sorted(maleNameCount.collect(), key = lambda t: t[1], reverse = True)[:5])\n",
    "print(\"top 5 name of female\", sorted(femaleNameCount.collect(), key = lambda t: t[1], reverse = True)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 3 name in top 10 cities [('ISTANBUL', ['MURAT YILMAZ', 'MURAT KAYA', 'MUSTAFA YILMAZ']), ('ANKARA', ['MURAT YILMAZ', 'MURAT SAHIN', 'MUSTAFA YILMAZ']), ('IZMIR', ['MUSTAFA YILMAZ', 'MURAT YILMAZ', 'MEHMET YILMAZ']), ('ADANA', ['MEHMET YILMAZ', 'MEHMET KAYA', 'MEHMET DEMIR']), ('GAZIANTEP', ['MEHMET YILMAZ', 'MEHMET KAYA', 'MEHMET YILDIRIM']), ('BURSA', ['MEHMET YILMAZ', 'AHMET YILMAZ', 'AYSE YILMAZ']), ('KAHRAMANMARAS', ['MEHMET DEMIR', 'MEHMET KOSE', 'FATMA DEMIR']), ('SANLIURFA', ['MEHMET DEMIR', 'FATMA DEMIR', 'MEHMET CIFTCI']), ('KONYA', ['MEHMET YILMAZ', 'MUSTAFA YILMAZ', 'MUSTAFA KAYA']), ('MALATYA', ['MEHMET DOGAN', 'MEHMET KAYA', 'MEHMET YILMAZ'])]\n"
     ]
    }
   ],
   "source": [
    "# N7. top 3 name in top 10 cities\n",
    "from heapq import nlargest\n",
    "from operator import add\n",
    "from collections import Counter\n",
    "def counterTupleList(l_:list):\n",
    "    dic = Counter(l_)\n",
    "    keys=dic.keys()\n",
    "    vals=dic.values()\n",
    "    tupleList=[(key,val) for key,val in zip(keys,vals)]\n",
    "    return tupleList\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_new.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "cityNamePair = data.map(lambda x :(x[7], \" \".join([x[2],x[3]]))) \n",
    "top10cityGroup = cityNamePair.groupByKey() \\\n",
    "                            .sortBy(lambda x : len(x[1]),ascending=False) \\\n",
    "                            .zipWithIndex() \\\n",
    "                            .filter(lambda x: x[1] < 10) \\\n",
    "                            .map(lambda x : x[0])\n",
    "top3name = top10cityGroup.mapValues(lambda x : counterTupleList(x)) \\\n",
    "                            .mapValues(lambda x : nlargest(3, x, key = lambda t : t[1])) \\\n",
    "                            .mapValues(lambda x : [t[0] for t in x])\n",
    "print(\"top 3 name in top 10 cities\", top3name.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.linalg import Vector,Vectors\n",
    "# from pyspark.sql import Row\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import IndexToString,StringIndexer,VectorIndexer\n",
    "# from pyspark import SparkConf,SparkContext\n",
    "# from pyspark.sql import SparkSession\n",
    "# import pyspark.sql as sql\n",
    "\n",
    "# data = sc.textFile('hdfs://localhost:9001/user/data_4000w.txt', 12).map(lambda x : re.split(\"\\t\", x))\n",
    "# # classOfFirstName = data.map(lambda x: x[2]).distinct().persist()\n",
    "# # print(classOfFirstName.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(classOfFirstName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classOfLastName = data.map(lambda x: x[3]).distinct().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import Row\n",
    "# spark = SparkSession(sc)\n",
    "# from pyspark.ml.classification import NaiveBayes\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "# spark.conf.set(\"spark.executor.memory\",\"250g\")\n",
    "# spark.conf.set(\"spark.driver.memory\",\"250g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def label2onehot(label,len_):\n",
    "#     onehot = [0] * len_\n",
    "#     onehot[int(label)] = 1\n",
    "#     return onehot\n",
    "    \n",
    "# # currentdata = data.map(lambda x: len(x))\n",
    "# # currentdata.take(10)\n",
    "# currentdata = data.map(lambda x: [x[11], x[14].split()[0]])\n",
    "# print(currentdata.take(10))\n",
    "# # len1 = data.map(lambda x: x[2]).distinct().count() \n",
    "# # len2 = data.map(lambda x: x[3]).distinct().count()\n",
    "# len1 = 627504 #  data.map(lambda x: x[2]).distinct().count() \n",
    "# len2 = 332884 #  data.map(lambda x: x[3]).distinct().count()\n",
    "# print(data.map(lambda x: x[14]).distinct().count())\n",
    "# print(data.map(lambda x: x[14].split()[0]).distinct().count())\n",
    "# schema = ['city','street_address']\n",
    "# df = spark.createDataFrame(currentdata, schema)\n",
    "\n",
    "# indexer = [StringIndexer(inputCol = column, outputCol = column + \"_index\") for column in schema]\n",
    "\n",
    "# pipeline = Pipeline(stages = indexer)\n",
    "# df = pipeline.fit(df).transform(df)\n",
    "# df = df.drop(*schema)\n",
    "\n",
    "# df.show()\n",
    "# # word2Vec = Word2Vec(vectorSize = 5, seed= 42,inputCol = \"lastname\")\n",
    "# # row1 = Row(\"lastname\")\n",
    "# # embed1 = word2Vec.fit(classOfLastName.map(lambda x : [x] if x else ['0']).map(row1).toDF())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classOfFirstName = data.map(lambda x: x[2]).distinct().count() \n",
    "# classOfLastName = data.map(lambda x: x[3]).distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.regression import LabeledPoint\n",
    "# from pyspark.mllib.tree import DecisionTree \n",
    "# from pyspark.mllib.tree import GradientBoostedTrees\n",
    "# from pyspark.mllib.tree import RandomForest\n",
    "# from pyspark.mllib.classification import NaiveBayes\n",
    "# from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.feature import StringIndexer\n",
    "# import time\n",
    "\n",
    "\n",
    "# Data = df.rdd.map(lambda row: LabeledPoint(row[0], row[1:2]))\n",
    "# print(Data.take(10))\n",
    "\n",
    "# trainData, validData, testData = Data.randomSplit([0.7,0.1,0.2])\n",
    "\n",
    "# trainData.persist()\n",
    "# validData.persist()\n",
    "# testData.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # classOfFirstName = 627504 #  data.map(lambda x: x[2]).distinct().count() \n",
    "# # classOfLastName = 332884 #  data.map(lambda x: x[3]).distinct().count()\n",
    "# classOfFirstName = data.map(lambda x: x[2]).distinct().count() \n",
    "# classOfLastName = data.map(lambda x: x[3]).distinct().count()\n",
    "# print(classOfFirstName, classOfLastName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy(model, validData):\n",
    "#     predict = model.predict(validData.map(lambda p: p.features))\n",
    "#     predict = predict.map(lambda p : float(p))\n",
    "#     correct = predict.zip(validData.map(lambda x: x.label)).filter(lambda x: x[0] == x[1])\n",
    "#     acc = float(correct.count())/float(predict.count())\n",
    "#     return acc\n",
    "\n",
    "# def trainEvaluateModel(trainData, validData, impurityParm, maxDepthParm, maxBinsParm, categoricalFeature):\n",
    "#     print(maxBinsParm)\n",
    "#     startTime = time.clock()\n",
    "# #     model = DecisionTree.trainClassifier(trainData, numClasses = 2, categoricalFeaturesInfo = categoricalFeature, impurity = impurityParm, maxDepth = maxDepthParm, maxBins = maxBinsParm) \n",
    "#     model = RandomForest.trainClassifier(trainData, 2, {}, 3, seed=42)\n",
    "#     acc = accuracy(model, validData)\n",
    "#     duration = time.clock() - startTime\n",
    "#     print(\"impur \"+str(impurityParm),\" maxDepth \"+str(maxDepthParm)+\" maxBins \"+str(maxBinsParm)+\" time \"+str(duration),\" accuracy \"+str(acc))\n",
    "#     return acc, duration, impurityParm, maxDepthParm, maxBinsParm\n",
    "\n",
    "# impurityParm = \"gini\"\n",
    "# def gridSearch(trainData, validData, impurityParm, maxDepthList = [9,12,24], maxBinsList = [727504,1000000,2000000]):\n",
    "#     metrics = [trainEvaluateModel(trainData, validData, impurityParm, maxDepth, maxBins, dict()) for maxDepth in maxDepthList for maxBins in maxBinsList]\n",
    "#     sorted_ =  sorted(metrics, key = lambda k: k[0], reverse = True)\n",
    "#     best = sorted_[0]\n",
    "#     print(\"best param, maxdepth: \"+str(best[3])+\" maxbins \"+ str(best[4]))\n",
    "#     return best\n",
    "\n",
    "# best = gridSearch(trainData, validData, impurityParm)\n",
    "# # trainEvaluateModel(trainData, testData, best[2], best[3], best[4], {})\n",
    "\n",
    "# trainEvaluateModel(trainData, testData, best[2], best[3], best[4], {0:classOfFirstName, 1:classOfLastName})\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # logistic regression\n",
    "# def accuracy(model, validData):\n",
    "#     predict = model.predict(validData.map(lambda p: p.features))\n",
    "#     predict = predict.map(lambda p : float(p))\n",
    "#     correct = predict.zip(validData.map(lambda x: x.label)).filter(lambda x: x[0] == x[1])\n",
    "#     acc = float(correct.count())/float(predict.count())\n",
    "#     return acc\n",
    "\n",
    "# def trainEvaluateModel(trainData, testData):\n",
    "#     print(maxBinsParm)\n",
    "#     startTime = time.clock()\n",
    "#     model =  LogisticRegressionWithSGD.train(trainData, iterations=10)\n",
    "#     acc = accuracy(model, testData)\n",
    "#     duration = time.clock() - startTime\n",
    "#     print(\" accuracy \"+str(acc))\n",
    "#     return acc, duration\n",
    "# trainEvaluateModel(trainData, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy(model, validData):\n",
    "#     predict = model.predict(validData.map(lambda p: p.features))\n",
    "#     predict = predict.map(lambda p : float(p))\n",
    "#     correct = predict.zip(validData.map(lambda x: x.label)).filter(lambda x: x[0] == x[1])\n",
    "#     acc = float(correct.count())/float(predict.count())\n",
    "#     return acc\n",
    "\n",
    "# def trainEvaluateModel(trainData, testData):\n",
    "#     startTime = time.clock()\n",
    "#     model =  NaiveBayes.train(trainData)\n",
    "#     acc = accuracy(model, testData)\n",
    "#     duration = time.clock() - startTime\n",
    "#     print(\" accuracy: \"+str(acc)+\" during \"+str(duration))\n",
    "#     return acc, duration, model\n",
    "# acc, duration, NBmodel = trainEvaluateModel(trainData, testData)\n",
    "# accuracy(NBmodel, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citycount = data.map(lambda x: x[11]).distinct().count()\n",
    "# def accuracy(model, validData):\n",
    "#     predict = model.predict(validData.map(lambda p: p.features))\n",
    "#     predict = predict.map(lambda p : float(p))\n",
    "#     correct = predict.zip(validData.map(lambda x: x.label)).filter(lambda x: x[0] == x[1])\n",
    "#     acc = float(correct.count())/float(predict.count())\n",
    "#     return acc\n",
    "\n",
    "# def trainEvaluateModel(trainData, testData):\n",
    "#     startTime = time.clock()\n",
    "#     model =  RandomForest.trainClassifier(trainData,citycount, {}, 3, seed=42)\n",
    "#     acc = accuracy(model, testData)\n",
    "#     duration = time.clock() - startTime\n",
    "#     print(\" accuracy: \"+str(acc)+\" during \"+str(duration))\n",
    "#     return acc, duration, model\n",
    "# acc, duration, model = trainEvaluateModel(trainData, testData)\n",
    "# accuracy(model, testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hit_n(l_, n):\n",
    "#     count = 0\n",
    "#     record = {}\n",
    "#     for item in l_:\n",
    "#         record[item] = item\n",
    "#         count += 1\n",
    "#     record  = sorted(record.items() , key = lambda x:x[1], reverse = True)\n",
    "#     result = []\n",
    "#     for item in record:\n",
    "#         result.append(item[0])\n",
    "#     return result\n",
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "# from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# vectorForm = df.rdd.map(lambda row: (row[0], Vectors.dense(row[1:2])))\n",
    "# Vectordf = spark.createDataFrame(vectorForm, ['label', 'features'])\n",
    "# trainData, validData, testData = Vectordf.rdd.randomSplit([0.7,0.1,0.2])\n",
    "# rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=3)\n",
    "# rfmodel = rf.fit(trainData.toDF())\n",
    "# predicitons = rfmodel.transform(testData.toDF())\n",
    "# p1 = predictions.select('probability','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(out.take(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
