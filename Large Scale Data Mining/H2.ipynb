{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects of using Spark\n",
    "#### 16300200020 Yanjian Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the linux command to delete the first 76 lines and redirect to the data_new.txt file.Then delete the last 49 lines and redirect to the data_new.txt. We can see that the total numbers of lines are 49611709"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tail -n +76 data_dump.sql > data_tmp.txt\n",
    "wc -l data_tmp.txt    result : 49611758\n",
    "head -49611709 data_tmp.txt > data_new.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce the spark package and configure the spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bigdatalab06'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function,division\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from itertools import islice\n",
    "import re\n",
    "conf = SparkConf().setAppName(\"turkey\").setMaster(\"local\").set(\"spark.executor.memory\",\"12g\")\n",
    "conf.set(\"spark.executor.cores\",\"4\").set(\"spark.memory.offHeap.enabled\",\"true\")\n",
    "conf.set(\"spark.executor.memoryOverhead\",\"4096\").set(\"spark.memory.offHeap.size\",\"4096\")\n",
    "sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vector,Vectors\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer,VectorIndexer\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql as sql\n",
    "\n",
    "data = sc.textFile('hdfs://localhost:9001/user/data_4000w.txt').map(lambda x : re.split(\"\\t\", x))\n",
    "# classOfFirstName = data.map(lambda x: x[2]).distinct().persist()\n",
    "# print(classOfFirstName.take(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(classOfFirstName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classOfLastName = data.map(lambda x: x[3]).distinct().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "spark = SparkSession(sc)\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['MALATYA', 'BOGAZICI'], ['MALATYA', 'CITILBAGI'], ['MALATYA', 'BOGAZICI'], ['MALATYA', 'KALE'], ['MALATYA', 'CITILBAGI'], ['MALATYA', 'CITILBAGI'], ['MALATYA', 'KALE'], ['MALATYA', 'KALE'], ['MALATYA', 'KALE'], ['MALATYA', 'BOGAZICI']]\n",
      "+----------+--------------------+\n",
      "|city_index|street_address_index|\n",
      "+----------+--------------------+\n",
      "|      24.0|               809.0|\n",
      "|      24.0|             92338.0|\n",
      "|      24.0|               809.0|\n",
      "|      24.0|                79.0|\n",
      "|      24.0|             92338.0|\n",
      "|      24.0|             92338.0|\n",
      "|      24.0|                79.0|\n",
      "|      24.0|                79.0|\n",
      "|      24.0|                79.0|\n",
      "|      24.0|               809.0|\n",
      "|      24.0|                79.0|\n",
      "|      24.0|                79.0|\n",
      "|      24.0|                79.0|\n",
      "|      24.0|                79.0|\n",
      "|      24.0|               809.0|\n",
      "|      24.0|               809.0|\n",
      "|      24.0|             92338.0|\n",
      "|      24.0|                79.0|\n",
      "|      24.0|             92338.0|\n",
      "|      24.0|               809.0|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "currentdata = data.map(lambda x: [x[11], x[14].split()[0]])\n",
    "print(currentdata.take(10))\n",
    "\n",
    "# print(data.map(lambda x: x[14]).distinct().count()) # 207482\n",
    "# print(data.map(lambda x: x[14].split()[0]).distinct().count()) # 119346\n",
    "schema = ['city','street_address']\n",
    "df = spark.createDataFrame(currentdata, schema)\n",
    "\n",
    "indexer = [StringIndexer(inputCol = column, outputCol = column + \"_index\") for column in schema]\n",
    "\n",
    "pipeline = Pipeline(stages = indexer)\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.drop(*schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "vectorForm = df.rdd.map(lambda row: (row[0], Vectors.dense(row[1:2])))\n",
    "Vectordf = spark.createDataFrame(vectorForm, ['label', 'features'])\n",
    "trainData, validData, testData = Vectordf.rdd.randomSplit([0.7,0.1,0.2])\n",
    "# randomforest\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=3)\n",
    "rfmodel = rf.fit(trainData.toDF())\n",
    "predictions = rfmodel.transform(testData.toDF())\n",
    "\n",
    "p1 = predictions.select('probability', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = predictions.select('probability', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         probability|label|\n",
      "+--------------------+-----+\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.06093868190131...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "|[0.28781466500985...| 24.0|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p1.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22996436894098976, 0.07439285909703836, 0.04869332379240792, 0.04343798619180908, 0.038241862958964444]\n"
     ]
    }
   ],
   "source": [
    "# print(p1.rdd.map(lambda x: (list(enumerate(x[0])), x[1])).take(10))\n",
    "hits = p1.rdd.map(lambda x: (sorted(enumerate(x[0]), key = lambda y: -y[1]), x[1])).map(lambda row: ([x[0] for x in row[0]], row[1])) # 按index 排序\n",
    "hits = hits.map(lambda x: [ 1 if x[0][i]== x[1] else 0 for i in range(5)])\n",
    "total = hits.count()\n",
    "hits = [hits.filter(lambda x: x[i] == 1).count()/total for i in range(5)]\n",
    "print(hits)\n",
    "# hits.take(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topn = [p1.rdd.map(lambda x: (hit_n(x[0],1),x[1])).filter(lambda x: int(x[1]) in x[0]).count()/float(top1_pre.count()) for i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(top1_pre.take(10))\n",
    "# print(top1_pre.filter(lambda x: int(x[1]) in x[0]).count()/float(top1_pre.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
